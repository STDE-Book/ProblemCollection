\ifspanish

\question Un problema de decisión binaria bidimensional viene caracterizado por la equiprobabilidad de las hipótesis y por las verosimilitudes
$$
\begin{array}{ll}
p_{X_1,X_2|H}(x_1,x_2 |0) = K_0 x_1 (1-x_2), & \quad  0 \le x_1 \le 1, \quad  0 \le x_2 \le 1  \\
p_{X_1,X_2|H}(x_1,x_2 |1) = K_1 x_1 x_2,     & \quad  0 \le x_1 \le 1, \quad  0 \le x_2 \le 1  
\end{array}
$$

\begin{parts}
 	\part Calcule los valores de las constantes $K_0$ y $K_1$.
 	\part Establezca el decisor de mínima probabilidad de error, e indique el carácter de los estadísticos $X_1$ y $X_2$.
 	\part Determine las ddp marginales $p_{X_{i}|H}(x_{i}|j)$, $i=1,2$ y $j=0,1$. ¿Qué relación estadística hay entre $X_1$ y $X_2$ bajo cada hipótesis? 
 	\part Calcule $P_{\rm FA}$, $P_{\rm M}$ y $P_{\rm e}$.
 	\part En la práctica, la medida de $X_2$ viene acompañada de un ruido aditivo $N$ independiente de $X_1$, $X_2$ y $H$; es decir, se observa $Y= X_2 + N$. Diseñe el decisor óptimo para esta situación cuando la ddp de este ruido tiene la forma: 
$$p_N(n) = 1, \quad 0 \le n \le 1$$
	\part Calcule $P_{\rm FA}'$, $P_{\rm M}'$ y $P_{e}'$ para la situación y el diseño del apartado anterior.
 \end{parts}

%%%%%%%%%%%%%%%%
\begin{solution}

\begin{parts}
\part Dado que las verosimilitudes son densidades de probabilidad, su integral debe ser unitaria
\begin{align*}
\int_0^1\int_0^1 & p_{X_1,X_2|H}(x_1,x_2 | 0) dx_1 dx_2 = 1        \\
& \Rightarrow \quad K_0 \int_0^1\int_0^1 x_1 (1-x_2) dx_1 dx_2 = 1   \\
& \Rightarrow \quad K_0 \int_0^1 x_1 dx_1 \int_0^1(1-x_2) dx_2= 1   \\
& \Rightarrow \quad K_0 = 4
\end{align*}
Análogamente, resulta
\begin{align*}
\int_0^1\int_0^1 p_{X_1,X_2|H}(x_1,x_2 | 1) dx_1 dx_2 = 1  \quad \Rightarrow \quad K_1 = 4
\end{align*}

\part El decisor de mínima probabilidad de error es el decisor MAP, dado por
\begin{align*}
P_H(1) & p_{X_1,X_2|H}(x_1,x_2 | 1) dx_1 dx_2 \dunodcero  P_H(0) p_{X_1,X_2|H}(x_1,x_2 | 0) dx_1 dx_2       \\
& \Leftrightarrow \quad 4 x_1 x_2 \dunodcero 4 x_1 (1-x_2)    \\
& \Leftrightarrow \quad x_2 \dunodcero (1-x_2)    \\
& \Leftrightarrow \quad x_2 \dunodcero \frac12
\end{align*}
Se observa que $X_1$ es irrelevante para la decisión y $X_2$ es un estadístico suficiente.

\part Es inmediato comprobar que ambas verosimilitudes son factorizables como producto de dos densidades de probabilidad, una por cada variable:
\begin{align*}
p_{X_1,X_2|H}(x_1,x_2 |0) &= (2 x_1) \cdot (2 (1-x_2)), & \quad  0 \le x_1 \le 1, \quad  0 \le x_2 \le 1  \\
p_{X_1,X_2|H}(x_1,x_2 |1) &= (2 x_1) \cdot (2 x_2),     & \quad  0 \le x_1 \le 1, \quad  0 \le x_2 \le 1  
\end{align*}
Por tanto, $X_1$ y $X_2$ son independientes, y las distribuciones marginales son estos factores.
\begin{align*}
p_{X_1|H}(x_1|0) &= 2 x_1,     \qquad 0 \le x_1 \le 1   \\
p_{X_2|H}(x_2|0) &= 2 (1-x_2), \qquad 0 \le x_2 \le 1; \\
p_{X_1|H}(x_1|1) &= 2 x_1,     \qquad 0 \le x_1 \le 1  \\
p_{X_2|H}(x_2|1) &= 2 x_2,     \qquad 0 \le x_2 \le 1  
\end{align*}

(En cualquier caso, siempre puede calcular las distribuciones marginales por el procedimiento general. Así, por ejemplo,
\begin{align*}
p_{X_1|H}(x_1|0) &= \int_0^1 p_{X_1,X_2|H}(x_1,x_2 | 0) dx = 4x_1 \int_0^1 (1-x_2) dx_2 = 2x_1,    \qquad 0 \le x_1 \le 1 
\end{align*}
que coincide con el resultado mostrado anteriormente. De modo análogo se podría proceder con el resto de distribuciones).

\part  
\begin{align*}
\pfa  &= P\{D=1 | H=0\} = P\left\{ X_2 > \frac12 | H=0 \right\} 
       = \int_{\frac12}^1 p_{X_2|H}(x_2|0) dx_2 \\
      &= \int_{\frac12}^1 2 (1-x_2) dx_2 = \frac14   \\
\pmis &= P\{D=0 | H=1\} = P\left\{ X_2 < \frac12 | H=1 \right\} 
       = \int_0^{\frac12} p_{X_2|H}(x_2|1) dx_2   \\
      &= \int_0^{\frac12} 2 x_2 dx_2 = \frac14  \\
P_{\rm e} &= P_H(0) \pfa + P_H(1) \pmis = \frac14
\end{align*}

\part El decisor MAP basado en $X_1$ e $Y$ estará dado por
\begin{align*}
p_{X_1, Y | H}(x_1,y | 0) \dunodcero p_{X_1, Y | H}(x_1,y |1) 
\end{align*}
Dado que $Y$ solamente depende de $X_2$ y $N$, y dado que éstas son independientes de $X_1$, se concluye que $Y$ también es independiente de $X_1$. Por tanto, el decisor MAP también puede escribirse como
\begin{align*}
p_{X_1 | H}(x_1 | 1) & p_{Y | H}(y | 1)  \dunodcero p_{X_1 | H}(x_1 |0)  p_{Y | H}(y |0)   \\
\Leftrightarrow & \quad p_{Y | H}(y | 1)  \dunodcero p_{Y | H}(y |0)
\end{align*}
Como $Y$ es suma de dos variables independientes, su ddp será convolución de las ddps de cada una de ellas. Por tanto
\begin{align*}
p_{Y|H}(y|0) 
	&= p_{X_2|H}(y|0) * p_{N|H}(y|0)
    = p_{X_2|H}(y|0) * p_{N}(y)   \\
	&= \left[\begin{array}{ll}
	         2 (1-y),  & y \in [0, 1]     \\
	         0,        & y \notin [0, 1]
            \end{array}
       \right] * 
       \left[\begin{array}{ll}
	   1, & y \in [0, 1]     \\
       0, & y \notin [0, 1]
       \end{array} \right]   \\
	&= \left[\begin{array}{ll}
		     2y-y^2,   & \quad 0 \le y \le 1 \\
             4-4y+y^2, & \quad 1 < y \le 2    \\
             0,        & \quad y \notin [0, 2]
             \end{array}
        \right] 
\end{align*}
Análogamente
\begin{align*}
p_{Y|H}(y|1)
	&= p_{X_2|H}(y|0) * p_{N}(y)  
	= \left[\begin{array}{ll}
	         2 y, & y \in [0, 1]     \\
	         0,   & y \notin [0, 1]
            \end{array}
       \right] * 
       \left[\begin{array}{ll}
	   1, & y \in [0, 1]     \\
       0, & y \notin [0, 1]
       \end{array}
	   \right]   \\
	&= \left[\begin{array}{ll}
	         y^2,      & \quad 0 \le y \le 1 \\
             2y - y^2, & \quad 1 <y \le 2    \\
             0,        & \quad y \notin [0, 2]
             \end{array}
        \right] 
\end{align*}
Por tanto, el decisor MAP será
\begin{align*}
& 
\left[\begin{array}{ll}
	  y^2,      & \quad 0 \le y \le 1 \\
      2y - y^2, & \quad 1 <y \le 2    \\
      0,        & \quad y \notin [0, 2]
      \end{array} \right]
\dunodcero
\left[\begin{array}{ll}
      2y-y^2,   & \quad 0 \le y \le 1 \\
      4-4y+y^2, & \quad 1 <y \le 2    \\
      0,        & \quad y \notin [0, 2]
      \end{array}\right]    \\
& \Leftrightarrow
\left[\begin{array}{ll}
	  y^2       \dunodcero 2y - y^2   & \quad 0 \le y \le 1 \\
      2y - y ^2 \dunodcero 4-4y+y^2,  & \quad 1 <y \le 2
      \end{array}\right]    \\
& \Leftrightarrow
\left[\begin{array}{ll}
	  y  \dunodcero 1            & \quad 0 \le y \le 1 \\
      0  \dunodcero (2-y)(1-y),  & \quad 1 <y \le 2
      \end{array}\right]     \\
& \Leftrightarrow
\left[\begin{array}{ll}
	  D = 0   & \quad 0 \le y \le 1 \\
      D = 1,  & \quad 1 <y \le 2
      \end{array}\right] \\
& \Leftrightarrow
y \dunodcero 1
\end{align*}
\part 
\begin{align*}
\pfa'  &= P_{D|H}(1|0) = P\left\{ Y > 1 | H=0 \right\} 
        = \int_1^2 p_{Y|H}(y|0) dy 
        = \int_1^2 \left(4-4y+y^2\right) dy = \frac13   \\
\pmis' &= P_{D|H}(0|1) = P\left\{ Y < 1 | H=1 \right\} 
        = \int_0^1 p_{Y|H}(y|1) dy   
        = \int_0^1 2y-y^2 dy = \frac13  \\
P_{\rm e} &= P_H(0) \pfa + P_H(1) \pmis = \frac13
\end{align*}
\end{parts}
\end{solution}

\else

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question A bidimensional binary decision probability is characterized by equally probable hypotheses, and likelihoods:
$$
\begin{array}{ll}
p_{X_1,X_2|H}(x_{1},x_{2}|0) = K_{0}x_{1}(1-x_2),& \quad	0<x_1,x_2<1 \\
p_{X_1,X_2|H}(x_{1},x_{2}|1) = K_{1}x_{1}x_2,&     \quad	0<x_1,x_2<1 \\
\end{array}
$$
$(K_0,K_1>0)$.
\begin{parts}
\part Compute the values of constants $K_0$ and $K_1$.
\part Find the classifier that minimizes the probability of error, and indicate the importance of $X_1$ and $X_2$ in the decision process.
\part Obtain marginal likelihoods $p_{X_{i}|H}(x_{i}|j)$, for $i=1,2$ and $j=0,1$. What is the statistical relationship between $X_1$ and $X_2$ under each hypothesis? 
\part Compute $P_{\rm FA}$, $P_{\rm M}$ y $P_{\rm e}$.
\part In practice, $X_2$ can not be observed directly, but we can just access a version contaminated with an additive noise $N$ independent of $X_1$, $X_2$ and $H$; i.e., we observe $Y= X_2 + N$. Design the optimal decision-maker for this situation when the noise pdf is: 
$$p_N(n) = 1, \quad 0<n<1$$
\part Compute $P_{\rm FA}'$, $P_{\rm M}'$ and $P_{e}'$ for the new situation and the classifier designed in part (e).
\end{parts}

%%%%%%%%%%%%%%%%
\begin{solution}

\begin{parts}
\part Given that the likelihoods are probability density functions, their integrals must be unity
\begin{align*}
\int_0^1\int_0^1 & p_{X_1,X_2|H}(x_1,x_2 | 0) dx_1 dx_2 = 1        \\
& \Rightarrow \quad K_0 \int_0^1\int_0^1 x_1 (1-x_2) dx_1 dx_2 = 1   \\
& \Rightarrow \quad K_0 \int_0^1 x_1 dx_1 \int_0^1(1-x_2) dx_2= 1   \\
& \Rightarrow \quad K_0 = 4
\end{align*}
In a similar way
\begin{align*}
\int_0^1\int_0^1 p_{X_1,X_2|H}(x_1,x_2 | 1) dx_1 dx_2 = 1  \quad \Rightarrow \quad K_1 = 4
\end{align*}

\part The classifier with minimum error probability is MAP, dado por
\begin{align*}
P_H(1) & p_{X_1,X_2|H}(x_1,x_2 | 1) dx_1 dx_2 \dunodcero  P_H(0) p_{X_1,X_2|H}(x_1,x_2 | 0) dx_1 dx_2       \\
& \Leftrightarrow \quad 4 x_1 x_2 \dunodcero 4 x_1 (1-x_2)    \\
& \Leftrightarrow \quad x_2 \dunodcero (1-x_2)    \\
& \Leftrightarrow \quad x_2 \dunodcero \frac12
\end{align*}
Variable $X_1$ is irrelevant for the decision and  $X_2$ is a sufficient statistic.

\part It is easy to see that both likelihood can be factorized as the product of two probability density functions, one per each variable:
\begin{align*}
p_{X_1,X_2|H}(x_1,x_2 |0) &= (2 x_1) \cdot (2 (1-x_2)), & \quad  0 \le x_1 \le 1, \quad  0 \le x_2 \le 1  \\
p_{X_1,X_2|H}(x_1,x_2 |1) &= (2 x_1) \cdot (2 x_2),     & \quad  0 \le x_1 \le 1, \quad  0 \le x_2 \le 1  
\end{align*}
Therefore, $X_1$ and $X_2$ are independent, the marginal distributions are given by these factors
\begin{align*}
p_{X_1|H}(x_1|0) &= 2 x_1,     \qquad 0 \le x_1 \le 1   \\
p_{X_2|H}(x_2|0) &= 2 (1-x_2), \qquad 0 \le x_2 \le 1; \\
p_{X_1|H}(x_1|1) &= 2 x_1,     \qquad 0 \le x_1 \le 1  \\
p_{X_2|H}(x_2|1) &= 2 x_2,     \qquad 0 \le x_2 \le 1  
\end{align*}

(In any case, we can compute the marginal distributions following the general proceducre. For instance,
\begin{align*}
p_{X_1|H}(x_1|0) &= \int_0^1 p_{X_1,X_2|H}(x_1,x_2 | 0) dx = 4x_1 \int_0^1 (1-x_2) dx_2 = 2x_1,    \qquad 0 \le x_1 \le 1 
\end{align*}
which matches the result previously obtained. We could proceed with the other distributions in the same way).

\part  
\begin{align*}
\pfa  &= P\{D=1 | H=0\} = P\left\{ X_2 > \frac12 | H=0 \right\} 
       = \int_{\frac12}^1 p_{X_2|H}(x_2|0) dx_2 \\
      &= \int_{\frac12}^1 2 (1-x_2) dx_2 = \frac14   \\
\pmis &= P\{D=0 | H=1\} = P\left\{ X_2 < \frac12 | H=1 \right\} 
       = \int_0^{\frac12} p_{X_2|H}(x_2|1) dx_2   \\
      &= \int_0^{\frac12} 2 x_2 dx_2 = \frac14  \\
P_{\rm e} &= P_H(0) \pfa + P_H(1) \pmis = \frac14
\end{align*}

\part The MAP decisión-maker based on $X_1$ and $Y$ is given by
\begin{align*}
p_{X_1, Y | H}(x_1,y | 0) \dunodcero p_{X_1, Y | H}(x_1,y |1) 
\end{align*}
Since $Y$ depends on $X_2$ and $N$ only, and knowing that these variables are independent of $X_1$, we conclude that $Y$ is also independent of $X_1$. Thus, the MAP decision-maker can be written, also, as
\begin{align*}
p_{X_1 | H}(x_1 | 1) & p_{Y | H}(y | 1)  \dunodcero p_{X_1 | H}(x_1 |0)  p_{Y | H}(y |0)   \\
\Leftrightarrow & \quad p_{Y | H}(y | 1)  \dunodcero p_{Y | H}(y |0)
\end{align*}
Since $Y$ is the sum of two independent random variables, its pdf will be the convolution of the pdf's of each one of them. Therefore, 
\begin{align*}
p_{Y|H}(y|0) 
	&= p_{X_2|H}(y|0) * p_{N|H}(y|0)
    = p_{X_2|H}(y|0) * p_{N}(y)   \\
	&= \left[\begin{array}{ll}
	         2 (1-y),  & y \in [0, 1]     \\
	         0,        & y \notin [0, 1]
            \end{array}
       \right] * 
       \left[\begin{array}{ll}
	   1, & y \in [0, 1]     \\
       0, & y \notin [0, 1]
       \end{array}
	   \right]   \\
	&= \left[\begin{array}{ll}
		     2y-y^2,   & \quad 0 \le y \le 1 \\
             4-4y+y^2, & \quad 1 < y \le 2    \\
             0,        & \quad y \notin [0, 2]
             \end{array}
        \right] 
\end{align*}
In the same way,
\begin{align*}
p_{Y|H}(y|1)
	&= p_{X_2|H}(y|0) * p_{N}(y)  
	= \left[\begin{array}{ll}
	         2 y, & y \in [0, 1]     \\
	         0,   & y \notin [0, 1]
            \end{array}
       \right] * 
       \left[\begin{array}{ll}
	   1, & y \in [0, 1]     \\
       0, & y \notin [0, 1]
       \end{array}
	   \right]   \\
	&= \left[\begin{array}{ll}
	         y^2,      & \quad 0 \le y \le 1 \\
             2y - y^2, & \quad 1 <y \le 2    \\
             0,        & \quad y \notin [0, 2]
             \end{array}
        \right] 
\end{align*}
Thus, the MAP decision-maker will be
\begin{align*}
& 
\left[\begin{array}{ll}
	  y^2,      & \quad 0 \le y \le 1 \\
      2y - y^2, & \quad 1 <y \le 2    \\
      0,        & \quad y \notin [0, 2]
      \end{array} \right]
\dunodcero
\left[\begin{array}{ll}
      2y-y^2,   & \quad 0 \le y \le 1 \\
      4-4y+y^2, & \quad 1 <y \le 2    \\
      0,        & \quad y \notin [0, 2]
      \end{array}\right]    \\
& \Leftrightarrow
\left[\begin{array}{ll}
	  y^2       \dunodcero 2y - y^2   & \quad 0 \le y \le 1 \\
      2y - y ^2 \dunodcero 4-4y+y^2,  & \quad 1 <y \le 2
      \end{array}\right]    \\
& \Leftrightarrow
\left[\begin{array}{ll}
	  y  \dunodcero 1            & \quad 0 \le y \le 1 \\
      0  \dunodcero (2-y)(1-y),  & \quad 1 <y \le 2
      \end{array}\right]     \\
& \Leftrightarrow
\left[\begin{array}{ll}
	  D = 0   & \quad 0 \le y \le 1 \\
      D = 1,  & \quad 1 <y \le 2
      \end{array}\right] \\
& \Leftrightarrow
y \dunodcero 1
\end{align*}
\part 
\begin{align*}
\pfa'  &= P_{D|H}(1|0) = P\left\{ Y > 1 | H=0 \right\} 
        = \int_1^2 p_{Y|H}(y|0) dy 
        = \int_1^2 \left(4-4y+y^2\right) dy = \frac13   \\
\pmis' &= P_{D|H}(0|1) = P\left\{ Y < 1 | H=1 \right\} 
        = \int_0^1 p_{Y|H}(y|1) dy   
        = \int_0^1 2y-y^2 dy = \frac13  \\
P_{\rm e} &= P_H(0) \pfa + P_H(1) \pmis = \frac13
\end{align*}
\end{parts}
\end{solution}

\fi