\ifspanish

\question[25] % MLG

Se toma una medida de la tensión intantánea $X$ existente en un momento dado en un nodo de un circuito. En dicho nodo existe una componente de señal con valor $S$, contaminada por ruido gaussiano aditivo de media nula y varianza $v$, e independiente de la señal. A priori, el valor de $S$ sigue una densidad de probabilidad gaussiana de media y varianza unitarias.

\begin{parts}
\part Suponiendo conocida $v$, calcule el estimador de máxima verosimilitud de $S$, $\hat s_\text{ML}(x)$.
\part Calcule el error cuadrático medio en el que incurre el estimador $\hat s_\text{ML}(x)$.  
\part Calcule la verosimilitud de $v$ a la vista de $x$, $p_{X|v}(x|v)$.
\part Calcule el estimador de máxima verosimilitud de $v$, $\hat v_\text{ML}(x)$.
\end{parts}

\begin{solution}
\begin{parts}
\part De acuerdo con el enunciado:
\begin{align*}
X = S + R
\end{align*}
(donde $R$ es el ruido). $R$ es gaussiana de media 0 y varianza $v$, y $S$ es gaussiana de media 1 y varianza 1. Por tanto, $X|S$ es gaussiana, de media
\begin{align*}
m_{X|s} = \mathbb{E}\{X|s\}
	= \mathbb{E}\{S+R|s\}
	= \mathbb{E}\{S|s\} + \mathbb{E}\{R|s\} 
	= s + \mathbb{E}\{R\} = s.
\end{align*}
y varianza
\begin{align*}
\mathbb{E}\{(X-m_{X|s})^2|s\} 
        = \mathbb{E}\{(X-s)^2|s\}
        = \mathbb{E}\{R^2|s\} = v 
\end{align*}
luego
\begin{align*}
p_{X|S}(x|s) = \frac{1}{\sqrt{2 \pi v}} \exp\left( -\frac{(x-s)^2}{2v}\right)
\end{align*}
y, maximizando respecto a s
\begin{align*}
\hat s_\text{ML}(x)=x
\end{align*}
\part El MSE del estimador $s_\text{ML}$ es
\begin{align*}
\mathbb{E}\{(\hat{S}_\text{ML} - S)^2\}
	= \mathbb{E}\{(X-S)^2\} 
	= \mathbb{E}\{(S+R-S)2\} 
	= \mathbb{E}\{R^2\} = v
\end{align*}
\part $X$ es gausiana, de media
\begin{align*}
\mathbb{E}\{X\} = \mathbb{E}\{S\} + \mathbb{E}\{R\} = 1
\end{align*}
y varianza
\begin{align*}
\mathbb{E}\{(X-1)^2\} = \mathbb{E}\{(S-1)\} + \mathbb{E}\{R^2\} = v +1
\end{align*}
por tanto
\begin{align*}
p_{X}(x) = \frac{1}{\sqrt{2 \pi (v+1)}} \exp\left( -\frac{(x-1)^2}{2(v+1)}\right)
\end{align*}
($v$ es un parámetro determinista de la distribución de modo que $p_{X}(x) \equiv p_{X|v}(x|v)$
\part 
\begin{align*}
\hat v_\text{ML}(x) 
	&= \arg\max_v p_{X|v}(x|v)
	 = \arg\max_v \log(p_{X|v}(x|v))     \\
	&= \arg\max_v \left\{- \frac12 \log(2 \pi (v+1)) - \frac{(x-1)^2}{2(v+1)}
	             \right\}    \\
	&= \arg\min_v \left\{\log(v+1) + \frac{(x-1)^2}{v+1}
	             \right\}
\end{align*}
Nótese que el valor de $v$ debe ser no-negativo. El valor de $v$ que anula la derivada es
\begin{align*}
v = (x-1)^2 - 1
\end{align*}
que podría ser negativo. En tal caso, el estimador ML es el valor más próximo, es decir, 0. Por tanto,
\begin{align*}
\hat v_\text{ML}(x) 
	= \text{máx}[(x-1)^2 - 1, 0]
\end{align*}

\end{parts}
\end{solution}

\else

\question Let $X$ be a measurement of the instantaneous voltage at a circuit node. In such node exists a signal component with value $S$, contaminated by Gaussian aditive noise with mean zero and variance $v$, which is independent of $S$. A priori, $S$ follows a Gaussian pdf with both the mean and variance equal to 1.

\begin{parts}
\part Assuming $v$ is known, obtain the maximum likelihood estimator of $S$, $\hat s_\text{ML}(x)$.
\part Calculate mean square error incurred by estimator $\hat S_\text{ML}(x)$.  
\part Obtain the likelihood of $v$ given $x$, i.e. $p_{X|v}(x|v)$.
\part Calculate the maximum likelihood estimator of $v$, $\hat v_\text{ML}(x)$.
\end{parts}

\begin{solution}
\begin{parts}
\part According to the statement,
\begin{align*}
X = S + R
\end{align*}
(where $R$ is the noise). $R$ zero-mean Gaussian with variance $v$, a $S$ is unity-mean Gaussiana with variance 1. Therefore, $X|S$ is Gaussian, with mean
\begin{align*}
m_{X|s} = \mathbb{E}\{X|s\}
	= \mathbb{E}\{S+R|s\}
	= \mathbb{E}\{S|s\} + \mathbb{E}\{R|s\} 
	= s + \mathbb{E}\{R\} = s.
\end{align*}
and variance
\begin{align*}
\mathbb{E}\{(X-m_{X|s})^2|s\} 
        = \mathbb{E}\{(X-s)^2|s\}
        = \mathbb{E}\{R^2|s\} = v 
\end{align*}
Thus
\begin{align*}
p_{X|S}(x|s) = \frac{1}{\sqrt{2 \pi v}} \exp\left( -\frac{(x-s)^2}{2v}\right)
\end{align*}
and, maximizing with respecto to $s$,
\begin{align*}
\hat s_\text{ML}(x)=x
\end{align*}
\part The MSE of estimator $s_\text{ML}$ is
\begin{align*}
\mathbb{E}\{(\hat{S}_\text{ML} - S)^2\}
	= \mathbb{E}\{(X-S)^2\} 
	= \mathbb{E}\{(S+R-S)2\} 
	= \mathbb{E}\{R^2\} = v
\end{align*}
\part $X$ is Gaussian with mean
\begin{align*}
\mathbb{E}\{X\} = \mathbb{E}\{S\} + \mathbb{E}\{R\} = 1
\end{align*}
and variance
\begin{align*}
\mathbb{E}\{(X-1)^2\} = \mathbb{E}\{(S-1)\} + \mathbb{E}\{R^2\} = v +1
\end{align*}
Therefore,
\begin{align*}
p_{X}(x) = \frac{1}{\sqrt{2 \pi (v+1)}} \exp\left( -\frac{(x-1)^2}{2(v+1)}\right)
\end{align*}
($v$ is a deterministic parameter of the distribution, so that $p_{X}(x) \equiv p_{X|v}(x|v)$
\part 
\begin{align*}
\hat v_\text{ML}(x) 
	&= \arg\max_v p_{X|v}(x|v)
	 = \arg\max_v \log(p_{X|v}(x|v))     \\
	&= \arg\max_v \left\{- \frac12 \log(2 \pi (v+1)) - \frac{(x-1)^2}{2(v+1)}
	             \right\}    \\
	&= \arg\min_v \left\{\log(v+1) + \frac{(x-1)^2}{v+1}
	             \right\}
\end{align*}
Nóte that $v$ is non-negative. The value of $v$ with zero-derivative of the log-likelihood is
\begin{align*}
v = (x-1)^2 - 1
\end{align*}
but it could be negative. In that case, the ML estimator is the closest non-negative value, that is, 0. Thus,
\begin{align*}
\hat v_\text{ML}(x) 
	= \text{máx}[(x-1)^2 - 1, 0]
\end{align*}

\end{parts}
\end{solution}

\fi