\ifspanish

\question[25] % VGV

Se desea estimar el valor de la v.a. $S$ a partir de la observación de otra variable aleatoria $X$, de las que se conoce su distribución conjunta
$$p_{S,X}(s,x) = 4x,  \qquad 0 \le s \le x^2, ~~ 0 \le x \le 1$$
Obtenga:
\begin{parts}
 		\part El estimador de máxima verosimilitud de $S$ a la vista de $X$, $\hat S_\text{ML}$.
 		\part El estimador de máximo a posteriori de $S$ a la vista de $X$, $\hat S_\text{MAP}$. 
 		\part El estimador de mínimo error cuadrático medio de $S$ a la vista de $X$, $\hat S_\text{MMSE}$.
 		\part El estimador lineal de  mínimo error cuadrático medio de $S$ a la vista de $X$, $\hat S_\text{LMMSE}=w_0+w_1x$.
\end{parts}

\begin{solution}
\begin{parts}
\part 
The prior distribution is
\begin{align*}
p_S(s) &= \int_{-\infty}^{\infty} p_{S,X}(s,x) dx
	    = \int_{\sqrt{s}}^{1} 4x dx = 2(1 - s)   
\end{align*}
Therefore, the likelihood function is
\begin{align*}
p_{X|S}(x|s) &= \frac{p_{X,S}(x,s)}{p_{S}(s)} 
	          = \frac{x}{1-s}            \qquad 0 \le s \le x^2, \qquad 0 \le x \le 1
\end{align*}
which is an increasing function of $s$. Thus, the ML estimate is the maximum value of $s$ in the domain of the likelihood function, that is
$$\hat S_\text{ML} = X^2$$
\part 
\begin{align*}
\hat{S}_\text{MAP}
	= \argmax_s p_{S|X}(s|x) 
	= \argmax_s \frac{p_{S,X}(s, x)}{p_X(x)}  
	= \argmax_s p_{S,X}(s, x) 
	= \argmax_s 4x 
\end{align*}
Thus, $\hat S_\text{MAP}$ is not unique: any value of $s \in [0,X^2]$ is a MAP estimate.
\part
Since the marginal distribution is
\begin{align*}
p_X(x) &= \int_{-\infty}^{\infty} p_{S,X}(s,x) ds
	    = \int_0^{x^2} 4x ds = 4x^3
\end{align*}
the posterior distribution is
\begin{align*}
p_{S|X}(s|x) &= \frac{p_{X,S}(x,s)}{p_X(x)} 
	          = \frac{1}{x^2}            \qquad 0 \le s \le x^2 \le 1
\end{align*}
Therefore
\begin{align*}
\hat s_\text{MMSE}
	&= \mathbb{E}\{S|x\} 
	 = \int_{-\infty}^{\infty} s p_{S|X}(s|x) ds
	 = \int_0^{x^2} \frac{s}{x^2} ds = \frac{x^2}{2} 	 
\end{align*}
\part  $\hat S_\text{LMMSE} =  X-\frac{7}{15} $
\end{parts}
\end{solution}

\else

\question[25] % VGV

We want to estimate the value of a random variable $S$ using a random observation $X$, from which the joint probability distribution is known
$$p_{S,X}(s,x) = 4x,  \qquad 0 \le s \le x^2, \qquad 0 \le x \le 1$$
Obtain:
\begin{parts}
\part The maximum likelihood estimator of $S$ given $X$, $\hat S_\text{ML}$.
\part The maximum {\em a posteriori} estimator of $S$ given $X$, $\hat S_\text{MAP}$.
\part The minimum mean square error estimator of $S$ given $X$, $\hat S_\text{MMSE}$. 
\part The linear estimator of $S$, with minimum mean square error, given $X$, $\hat S_\text{LMMSE}=w_0+w_1x$.
\end{parts}

\begin{solution}
\begin{parts}
\part 
The prior distribution is
\begin{align*}
p_S(s) &= \int_{-\infty}^{\infty} p_{S,X}(s,x) dx
	    = \int_{\sqrt{s}}^{1} 4x dx = 2(1 - s)   
\end{align*}
Therefore, the likelihood function is
\begin{align*}
p_{X|S}(x|s) &= \frac{p_{X,S}(x,s)}{p_{S}(s)} 
	          = \frac{x}{1-s}            \qquad 0 \le s \le x^2, \qquad 0 \le x \le 1
\end{align*}
which is an increasing function of $s$. Thus, the ML estimate is the maximum value of $s$ in the domain of the likelihood function, that is
$$\hat S_\text{ML} = X^2$$
\part 
\begin{align*}
\hat{S}_\text{MAP}
	= \argmax_s p_{S|X}(s|x) 
	= \argmax_s \frac{p_{S,X}(s, x)}{p_X(x)}  
	= \argmax_s p_{S,X}(s, x) 
	= \argmax_s 4x 
\end{align*}
Thus, $\hat S_\text{MAP}$ is not unique: any value of $s \in [0,X^2]$ is a MAP estimate.
\part
Since the marginal distribution is
\begin{align*}
p_X(x) &= \int_{-\infty}^{\infty} p_{S,X}(s,x) ds
	    = \int_0^{x^2} 4x ds = 4x^3
\end{align*}
the posterior distribution is
\begin{align*}
p_{S|X}(s|x) &= \frac{p_{X,S}(x,s)}{p_X(x)} 
	          = \frac{1}{x^2}            \qquad 0 \le s \le x^2 \le 1
\end{align*}
Therefore
\begin{align*}
\hat s_\text{MMSE}
	&= \mathbb{E}\{S|x\} 
	 = \int_{-\infty}^{\infty} s p_{S|X}(s|x) ds
	 = \int_0^{x^2} \frac{s}{x^2} ds = \frac{x^2}{2} 	 
\end{align*}
\part  
$$\hat S_\text{LMMSE} = {\bf w}^\intercal {\bf Z}
$$
where ${\bf Z} = (1, X)^\intercal$
\begin{align*}
{\bf w} 
    &= {\bf R}_{\bf Z}^{-1}{\bf r}_{S{\bf Z}}  \\
{\bf R}_{\bf Z}
    &= \mathbb{E}\{{\bf Z}{\bf Z}^\intercal\} 
     = \begin{pmatrix} 
       1               & \mathbb{E}\{X\}   \\
       \mathbb{E}\{X\} & \mathbb{E}\{X^2\} 
       \end{pmatrix}  \\
{\bf r}_{S{\bf Z}}
    &= \mathbb{E}\{S{\bf Z}\} 
     = \begin{pmatrix} 
       \mathbb{E}\{S\}   \\  \mathbb{E}\{SX\} 
       \end{pmatrix}
\end{align*}
Noting that
\begin{align*}
\mathbb{E}\{X\}   &= \int_0^1 4x^4 dx = \frac45   \\
\mathbb{E}\{X^2\} &= \int_0^1 4x^5 dx = \frac23   \\
\mathbb{E}\{S\}   &= \int_0^1 2s(1-s) ds = \frac13   \\
\mathbb{E}\{SX\}
	&= \int_0^1 \mathbb{E}\{Sx|x\}p_X(x) dx 
     = \int_0^1 \frac{x^3}{2} 4x^3 dx = \frac27
\end{align*}
we get
\begin{align*}
{\bf w} 
    &= \begin{pmatrix} 
       1       & \frac45  \\
       \frac45 & \frac23 
       \end{pmatrix}^{-1} 
       \begin{pmatrix} 
       \frac13   \\  \frac27 
       \end{pmatrix}
     = \begin{pmatrix} 
       -\frac5{21} \\ \frac57 
       \end{pmatrix}
\end{align*}
Thus
$\hat S_\text{LMMSE} =  \frac57 X - \frac5{21} $
\end{parts}
\end{solution}

\fi