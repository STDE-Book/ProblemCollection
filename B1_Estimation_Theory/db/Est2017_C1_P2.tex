\ifspanish

\question[25]  %JAG

Se desea estimar una variable aleatoria $S$ a partir de otra $X$, siendo la relación entre ellas

$$X = S \cdot T$$

donde $S$ y $T$ son variables aleatorias e independientes, ambas uniformes entre 0 y 1.

\begin{parts}
	\part Calcule la media y la varianza de $S$.
	\part Obtenga el estimador de máxima verosimilitud de $S$ a partir de $X$, $\hat{S}_{\text{ML}}$.
	\part Represente el dominio de la distribución conjunta de $S$ y $X$. Obtenga asimismo la expresión de la densidad de probabilidad conjunta de $S$ y $X$, $p_{S,X}(s,x)$.
	\part Calcule la media de $X$ y su valor cuadrático medio. (Sugerencia: Puede resultarle más sencillo calcular dichos valores sin obtener $p_X(x)$ como resultado intermedio).
	\part Obtenga el estimador lineal de mínimo error cuadrático medio, $\hat{S}_\text{LMSE} = w_0^\ast + w^\ast X$.
%	\part Obtenga el estimador lineal de menor error cuadrático medio de la forma $\hat{S}_\text{b} = w_b^\ast X$.
	\part Represente los estimadores obtenidos en este problema sobre unos mismos ejes coordenados $X$-$S$, y discuta cuál de ellos incurre en un menor error cuadrático medio.
\end{parts}

\begin{solution}
\begin{parts}
\part $\mathbb{E}\{S\} = \frac{1}{2}$ y $v_x = \frac{1}{12}$
\part Dado que $\hat{s}_{\text{ML}} = \argmax_s p_{X|S}$, calcularemos en primer lugar la función de verosimilitud,
\begin{align*}
p_{X|S}(x|s)
	&= \frac{d}{dx}F_{X|S}(x|s) 
	 = \frac{d}{dx}P\{X \le x |S=s\}
	 = \frac{d}{dx}P\{ST \le x |S=s\}   \\
	&= \frac{d}{dx}P\left\{T \le \frac{x}{s} \right\}
	 = \frac{d}{dx}F_T\left(\frac{x}{s}\right) 
	 = \frac{1}{s} p_T\left(\frac{x}{s}\right)    \\
	&= \frac{1}{s},		\qquad \quad    0 \le x \le s \le 1 
\end{align*}
Por tanto,
\begin{align}
\hat{s}_{\text{ML}} = \argmax_s p_{X|S}(x|s) = \argmax_{s \in [x, 1]}  \frac1s = x
\end{align}
\part 
\begin{align*}
p_{S,X}(s,x) = p_{X|S}(x|s) p_{X}(x) = \frac{1}{s},  \qquad \qquad  0 \le x \le s \le 1
\end{align*}
\part 
\begin{align*}
& \EE\{X\} = \EE\{ST\} = \EE\{S\} \EE\{T\} = \frac{1}{4}    \\
& \EE\{X^2\} = \EE\{S^2T^2\} = \EE\{S^2\} \EE\{T^2\} = \frac{1}{9}  
\end{align*}
\part El estimador LMSE  es $\hat{S}_\text{LMSE} = w_0^\ast + w^\ast X = {{\bf w}^{\ast}}^\intercal {\bf Z}$ donde ${\bf Z} = (1, X)^\intercal$. Por tanto.
\begin{align*}
{\bf w}^\ast 
	&= {\bf R}_{\bf Z}^{-1}{\bf r}_{s{\bf z}} 
	 = \begin{pmatrix}
	   1        & \EE\{X\}  \\ 
	   \EE\{X\} & \EE\{X^2\}
	   \end{pmatrix}^{-1}
	   \begin{pmatrix} \EE\{S\} \\ \EE\{SX\} \end{pmatrix}
\end{align*}
Sabiendo que $\EE\{SX\}=\EE\{S^2\}\EE\{T\}= \frac16$, resulta
\begin{align}
{\bf w}^\ast 
	&= \begin{pmatrix}
	   1       & \frac14  \\ 
	   \frac14 & \frac19
	   \end{pmatrix}^{-1}
	   \begin{pmatrix} \frac12 \\ \frac16  \end{pmatrix}
	 = \frac17 \begin{pmatrix} 2 \\ 6   \end{pmatrix}
\end{align}
that is $\hat{S}_\text{LMSE} = \frac27 + \frac67 X$
%\part $w_b^\ast = 1.5$
\part Dado que los dos estimadores son lineales, necesariamente $\hat{S}_\text{LMSE}$ es el de menor error cuadrático medio.
\end{parts}
\end{solution}


\else
%
\question[25]  %JAG

We wish to estimate random variable $S$ from random variable $X$. They are related as

$$X = S \cdot T$$

where $S$ and $T$ are independent random variables, both uniformly distributed between 0 and 1.

\begin{parts}
	\part Obtain the mean and the variance of $S$.
	\part Obtain the maximum likelihood estimator of $S$ as a function of $X$, $\hat{S}_{\text{ML}}$.
	\part Plot the support of the joint distribution of $S$ and $X$. Calculate also the joint probability density function of $S$ and $X$, $p_{S,X}(s,x)$.
	\part Obtain the mean of $X$ and its mean quadratic value. (Hint: It may be convenient not to compute $p_X(x)$ as an intermediate result).
	\part Design the linear minimum mean square error estimator, $\hat{S}_\text{LMSE} = w_0^\ast + w^\ast X$.
%	\part Obtain the estimator with analytical shape $\hat{S}_\text{b} = w_b^\ast X$ which incurs in the smallest mean square error.
	\part Plot the estimators that have been designed in this problem on top of the same coordinate axis $X$-$S$, and discuss which of them incurs in the smallest mean square error.
\end{parts}

\begin{solution}
\begin{parts}
\part $\mathbb{E}\{S\} = \frac{1}{2}$ y $v_x = \frac{1}{12}$
\part Since $\hat{s}_{\text{ML}} = \argmax_s p_{X|S}$, we will compute first the likelihood function,
\begin{align*}
p_{X|S}(x|s)
	&= \frac{d}{dx}F_{X|S}(x|s) 
	 = \frac{d}{dx}P\{X \le x |S=s\}
	 = \frac{d}{dx}P\{ST \le x |S=s\}   \\
	&= \frac{d}{dx}P\left\{T \le \frac{x}{s} \right\}
	 = \frac{d}{dx}F_T\left(\frac{x}{s}\right) 
	 = \frac{1}{s} p_T\left(\frac{x}{s}\right)    \\
	&= \frac{1}{s},		\qquad \quad    0 \le x \le s \le 1 
\end{align*}
Thus,
\begin{align}
\hat{s}_{\text{ML}} = \argmax_s p_{X|S}(x|s) = \argmax_{s \in [x, 1]}  \frac1s = x
\end{align}
\part 
\begin{align*}
p_{S,X}(s,x) = p_{X|S}(x|s) p_{X}(x) = \frac{1}{s},  \qquad \qquad  0 \le x \le s \le 1
\end{align*}
\part 
\begin{align*}
& \EE\{X\} = \EE\{ST\} = \EE\{S\} \EE\{T\} = \frac{1}{4}    \\
& \EE\{X^2\} = \EE\{S^2T^2\} = \EE\{S^2\} \EE\{T^2\} = \frac{1}{9}  
\end{align*}
\part The LMSE estimate is $\hat{S}_\text{LMSE} = w_0^\ast + w^\ast X = {{\bf w}^{\ast}}^\intercal {\bf Z}$ donde ${\bf Z} = (1, X)^\intercal$. Thus,
\begin{align*}
{\bf w}^\ast 
	&= {\bf R}_{\bf Z}^{-1}{\bf r}_{s{\bf z}} 
	 = \begin{pmatrix}
	   1        & \EE\{X\}  \\ 
	   \EE\{X\} & \EE\{X^2\}
	   \end{pmatrix}^{-1}
	   \begin{pmatrix} \EE\{S\} \\ \EE\{SX\} \end{pmatrix}
\end{align*}
Knowing that $\EE\{SX\}=\EE\{S^2\}\EE\{T\}= \frac16$, we get
\begin{align}
{\bf w}^\ast 
	&= \begin{pmatrix}
	   1       & \frac14  \\ 
	   \frac14 & \frac19
	   \end{pmatrix}^{-1}
	   \begin{pmatrix} \frac12 \\ \frac16  \end{pmatrix}
	 = \frac17 \begin{pmatrix} 2 \\ 6   \end{pmatrix}
\end{align}
that is $\hat{S}_\text{LMSE} = \frac27 + \frac67 X$
%\part $w_b^\ast = 1.5$
\part Since both estimators are linear, necessarily $\hat{S}_\text{LMSE}$ has a smaller MSE.
\end{parts}

\end{solution}

\fi